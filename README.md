# nanoGPT-moderno

[![Licen√ßa: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Python: 3.9+](https://img.shields.io/badge/Python-3.9+-blue.svg)](https://www.python.org/downloads/)
[![PyTorch: 2.x](https://img.shields.io/badge/PyTorch-2.x-orange.svg)](https://pytorch.org/)

Um fork do `nanoGPT` de Karpathy, atualizado com a arquitetura e as melhores pr√°ticas de LLMs de 2024‚Äì2025, como RoPE, GQA, SwiGLU e Flash Attention.

Este projeto moderniza o `nanoGPT` para ser uma base de c√≥digo clara e flex√≠vel, focada em arquitetura robusta e infer√™ncia otimizada. √â a ferramenta ideal para quem quer aprender, experimentar e entender a ponte entre o GPT-2 cl√°ssico e arquiteturas modernas como Llama 3.

---

## üéØ Para Quem √© Este Projeto?

*   **Estudantes e Pesquisadores:** Uma base de c√≥digo limpa para entender na pr√°tica os componentes de Transformers modernos.
*   **Desenvolvedores:** Um ponto de partida s√≥lido e minimalista para prototipar e experimentar com novas arquiteturas.
*   **Entusiastas:** Para qualquer pessoa que queira treinar seu pr√≥prio "GPT" do zero com tecnologia de ponta.

## ‚ú® Destaques da Arquitetura

Esta vers√£o implementa otimiza√ß√µes cruciais que se tornaram padr√£o em modelos de linguagem de alta performance.

| Componente                 | Vers√£o Cl√°ssica (GPT-2)     | Vers√£o Moderna (nanoGPT-moderno)        |
| :------------------------- | :-------------------------- | :-------------------------------------- |
| **Embeddings Posicionais** | Absolutos (`wpe`)           | **RoPE** (Rotational Position Embeddings) |
| **Normaliza√ß√£o**           | `LayerNorm`                 | **RMSNorm** (Root Mean Square Norm)     |
| **Aten√ß√£o (Mecanismo)**    | MHA (Multi-Head Attention)  | **GQA** (Grouped-Query Attention)       |
| **Aten√ß√£o (Kernel)**       | Implementa√ß√£o manual        | **SDPA** (Flash/Efficient Attention)    |
| **Ativa√ß√£o (MLP)**         | `GELU`                      | **SwiGLU**                              |
| **Bias em Camadas Densas** | Com bias                    | **Sem bias** para maior estabilidade    |
| **Infer√™ncia**             | Recomputa√ß√£o completa       | **KV-cache** incremental                |
| **Sa√≠da**                  | Logits brutos               | **Logit soft-capping**                  |
| **Inicializa√ß√£o**          | Padr√£o GPT-2                | Otimizada para arquitetura moderna      |
| **Estrutura de C√≥digo**    | Monol√≠tica                  | Modular e extens√≠vel                    |

## üöÄ Comece a Usar em Minutos

###. Modelo
*  `https://huggingface.co/rianagario/GPT2-Mod-Arch`

###. Requisitos

*   Python 3.9+
*   PyTorch 2.x (com suporte a CUDA para melhor performance)
*   Depend√™ncias adicionais: `numpy`, `tiktoken`, `transformers`, `tqdm`, `mlflow` (opcional).

## Relat√≥rio T√©cnico

*  `https://www.overleaf.com/read/prvwjxcjfxfn#1ebc57`

## Resumo Resultado

Treinado do zero em um dataset de 2 GB, o modelo validou a efic√°cia da arquitetura modernizada ao aprender a gerar senten√ßas gramaticalmente corretas. No entanto, como esperado para o volume de dados, o treinamento levou a um overfitting, fazendo com que o modelo se especializasse excessivamente no conte√∫do do corpus. Isso se manifestou em uma forte tend√™ncia a associar conceitos a personagens espec√≠ficos, tornando-o um "gerador de hist√≥rias" com vi√©s narrativo, em vez de um modelo de linguagem generalista. O resultado sublinha que a performance da arquitetura √© diretamente limitada pela escala e diversidade do dataset de treinamento.

## Teste T√©cnico

![Teste](https://raw.githubusercontent.com/BayesTheory/GPT2-Mod-Arch/main/img/print.jpeg)

![Teste](https://raw.githubusercontent.com/BayesTheory/GPT2-Mod-Arch/main/img/print%20(2).jpeg)

