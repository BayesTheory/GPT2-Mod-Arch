# app_chat.py

import streamlit as st
import torch
import tiktoken
import time
import gc

# Importe suas classes de modelo e config
from model import GPT, GPTConfig

# --- CONFIGURA√á√ïES DA P√ÅGINA E DO MODELO ---
# st.set_page_config define o t√≠tulo que aparece na aba do navegador, o √≠cone e o layout
st.set_page_config(
    page_title="Meu GPT Pessoal",
    page_icon="üß†",
    layout="centered"
)

# CORRE√á√ÉO: Adicionado 'r' antes da string para tratar o caminho como "raw" (cru)
CHECKPOINT_PATH = r"C:\Users\Rian\Desktop\gpt\mlruns\706298003787278334\2ab5a67b5c35444eb888c19ca8f8be80\artifacts\model_00499.pt"
DEVICE = 'cpu'

# --- L√ìGICA DO MODELO (A mesma fun√ß√£o com cache, pois √© essencial) ---

@st.cache_resource
def load_model_and_tokenizer():
    """
    Carrega o modelo e o tokenizador uma √∫nica vez.
    """
    with st.spinner("Inicializando o c√©rebro digital... üß† Por favor, aguarde."):
        # Carregar o modelo na CPU
        checkpoint = torch.load(CHECKPOINT_PATH, map_location=DEVICE)
        gptconf = GPTConfig(**checkpoint['model_args'])
        model = GPT(gptconf)
        state_dict = checkpoint['model']
        
        unwanted_prefix = '_orig_mod.'
        for k, v in list(state_dict.items()):
            if k.startswith(unwanted_prefix):
                state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)
        model.load_state_dict(state_dict)
        
        model.eval()
        model.to(DEVICE)
        
        del checkpoint, state_dict
        gc.collect()
        
        # Carregar tokenizador
        tokenizer = tiktoken.get_encoding("gpt2")
        
    return model, tokenizer

# --- FUN√á√ÉO DE GERA√á√ÉO COM EFEITO DE STREAMING ---

def generate_response_stream(model, tokenizer, prompt_text, max_new_tokens, temperature):
    """
    Gera a resposta completa e depois a "transmite" palavra por palavra.
    """
    start_ids = tokenizer.encode(prompt_text, allowed_special={"<|endoftext|>"})
    x = (torch.tensor(start_ids, dtype=torch.long, device=DEVICE)[None, ...])
    
    # Gerar a resposta completa (ainda √© o passo lento)
    with torch.no_grad():
        y = model.generate(x, max_new_tokens=max_new_tokens, temperature=temperature)
        full_response_tokens = y[0].tolist()
    
    # Decodifica apenas a parte nova da resposta
    new_response_tokens = full_response_tokens[len(start_ids):]
    response_text = tokenizer.decode(new_response_tokens)
    
    # Simula o streaming palavra por palavra
    for word in response_text.split():
        yield word + " "
        time.sleep(0.05) # Pequeno atraso para o efeito de digita√ß√£o

# --- INTERFACE GR√ÅFICA (O FRONTEND BONITINHO) ---

# T√≠tulo principal
st.title("Meu GPT Pessoal üß†")
st.caption("Uma interface de chat elegante para o seu modelo de linguagem.")

# Carrega o modelo e o tokenizador (pode mostrar o spinner na primeira vez)
try:
    model, tokenizer = load_model_and_tokenizer()

    # Configura√ß√µes na barra lateral
    with st.sidebar:
        st.header("üõ†Ô∏è Configura√ß√µes")
        max_new_tokens = st.slider("M√°ximo de novos tokens:", 50, 1000, 150)
        temperature = st.slider("Temperatura (Criatividade):", 0.1, 1.0, 0.7, 0.05)
        if st.button("üóëÔ∏è Limpar Hist√≥rico"):
            st.session_state.messages = []
            st.rerun()

    # Inicializa o hist√≥rico de chat na "mem√≥ria" da sess√£o do Streamlit
    if "messages" not in st.session_state:
        st.session_state.messages = []

    # Exibe as mensagens do hist√≥rico a cada atualiza√ß√£o da tela
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

    # Campo de input do usu√°rio (fica fixo no final da p√°gina)
    if prompt := st.chat_input("Como posso te ajudar?"):
        # Adiciona a mensagem do usu√°rio ao hist√≥rico e exibe na tela
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.markdown(prompt)

        # Gera e exibe a resposta do assistente
        with st.chat_message("assistant"):
            # O spinner agora fica dentro do bal√£o de chat do assistente
            with st.spinner("Pensando..."):
                response_generator = generate_response_stream(model, tokenizer, prompt, max_new_tokens, temperature)
                # st.write_stream simula o efeito de digita√ß√£o
                full_response = st.write_stream(response_generator)
        
        # Adiciona a resposta completa do assistente ao hist√≥rico
        st.session_state.messages.append({"role": "assistant", "content": full_response})
        
        # Um pequeno efeito de comemora√ß√£o :)
        st.balloons()

except FileNotFoundError:
    st.error(f"ERRO: Arquivo do modelo n√£o encontrado em '{CHECKPOINT_PATH}'.")
    st.info("Verifique o caminho da vari√°vel `CHECKPOINT_PATH` no arquivo `app_chat.py`.")
except Exception as e:
    st.error(f"Ocorreu um erro inesperado: {e}")
    st.exception(e) # Mostra mais detalhes do erro para debug
